{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b745053",
   "metadata": {},
   "source": [
    "# Data Pre-processing for Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1344948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       d  HOBBIES_1_001_TX_1  HOBBIES_1_002_TX_1  HOBBIES_1_003_TX_1  \\\n",
      "0   d_1514                   0                   0                   0   \n",
      "1   d_1515                   0                   0                   0   \n",
      "2   d_1516                   3                   0                   0   \n",
      "3   d_1517                   0                   0                   0   \n",
      "4   d_1518                   0                   0                   0   \n",
      "\n",
      "id  HOBBIES_1_004_TX_1  HOBBIES_1_005_TX_1  HOBBIES_1_006_TX_1  \\\n",
      "0                    2                   0                   2   \n",
      "1                    3                   0                   0   \n",
      "2                    0                   0                   0   \n",
      "3                    0                   0                   0   \n",
      "4                    3                   3                   1   \n",
      "\n",
      "id  HOBBIES_1_007_TX_1  HOBBIES_1_008_TX_1  HOBBIES_1_009_TX_1  ...  \\\n",
      "0                    0                   2                   0  ...   \n",
      "1                    0                   0                   0  ...   \n",
      "2                    0                   8                   0  ...   \n",
      "3                    0                  10                   0  ...   \n",
      "4                    1                   0                   1  ...   \n",
      "\n",
      "id  FOODS_3_818_TX_3  FOODS_3_819_TX_3  FOODS_3_820_TX_3  FOODS_3_821_TX_3  \\\n",
      "0                  3                 1                 1                 0   \n",
      "1                  0                 1                 0                 0   \n",
      "2                  2                 1                 1                 0   \n",
      "3                  3                 0                 0                 0   \n",
      "4                  0                 1                 1                 1   \n",
      "\n",
      "id  FOODS_3_822_TX_3  FOODS_3_823_TX_3  FOODS_3_824_TX_3  FOODS_3_825_TX_3  \\\n",
      "0                  0                 0                 0                 0   \n",
      "1                  3                 1                 0                 2   \n",
      "2                  0                 0                 0                 0   \n",
      "3                  0                 0                 0                 0   \n",
      "4                  0                 0                 0                 0   \n",
      "\n",
      "id  FOODS_3_826_TX_3  FOODS_3_827_TX_3  \n",
      "0                  2                 7  \n",
      "1                  1                 0  \n",
      "2                  1                 2  \n",
      "3                  0                 0  \n",
      "4                  0                 0  \n",
      "\n",
      "[5 rows x 9148 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np   \n",
    "\n",
    "# Load the csv files\n",
    "sales_train_df = pd.read_csv('sales_train_validation.csv')\n",
    "calendar_df = pd.read_csv('calendar.csv')\n",
    "\n",
    "\n",
    "# Filter for only Texas stores: TX_1, TX_2, TX_3\n",
    "texas_stores = ['TX_1', 'TX_2', 'TX_3']\n",
    "sales_train_df = sales_train_df[sales_train_df['store_id'].isin(texas_stores)]\n",
    "\n",
    "\n",
    "# Define the day columns for the last 400 days from d_1913\n",
    "start_day = 1913 - 399  # Calculate the starting day (d_1913 - 399 days)\n",
    "day_columns = ['d_' + str(day) for day in range(start_day, 1914)]  # List of day columns from d_1514 to d_1913\n",
    "\n",
    "# Select only the necessary columns: id columns + last 400 days\n",
    "id_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "selected_columns = id_columns + day_columns\n",
    "sales_train_df= sales_train_df[selected_columns]\n",
    "\n",
    "\n",
    "# Remove \"_validation\" from the 'id' column\n",
    "sales_train_df['id'] = sales_train_df['id'].str.replace('_validation', '')\n",
    "\n",
    "# Remove specified columns from the sales_train_df\n",
    "sales_train_df_modified = sales_train_df.drop(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], axis=1)\n",
    "\n",
    "# Transpose the modified sales_train_df\n",
    "# To preserve the 'id' column, set it as index before transposing\n",
    "sales_train_df_transposed = sales_train_df_modified.set_index('id').transpose().reset_index()\n",
    "\n",
    "# Rename columns for merging\n",
    "sales_train_df_transposed.rename(columns={'index': 'd'}, inplace=True)\n",
    "\n",
    "print(sales_train_df_transposed.head())\n",
    "\n",
    "sales_train_df_transposed.to_csv('sales_transpose.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2177a5",
   "metadata": {},
   "source": [
    "# Feature Engineering for Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f886f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122652/2723853685.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_features[column_name + '_1d_lag_7d_MA'] = lagged_series.rolling(window=7).mean()\n",
      "/tmp/ipykernel_122652/2723853685.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_features[column_name + '_1d_lag_14d_MA'] = lagged_series.rolling(window=14).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        d  HOBBIES_1_001_TX_1  HOBBIES_1_002_TX_1  HOBBIES_1_003_TX_1  \\\n",
      "0  d_1514                   0                   0                   0   \n",
      "1  d_1515                   0                   0                   0   \n",
      "2  d_1516                   3                   0                   0   \n",
      "3  d_1517                   0                   0                   0   \n",
      "4  d_1518                   0                   0                   0   \n",
      "\n",
      "   HOBBIES_1_004_TX_1  HOBBIES_1_005_TX_1  HOBBIES_1_006_TX_1  \\\n",
      "0                   2                   0                   2   \n",
      "1                   3                   0                   0   \n",
      "2                   0                   0                   0   \n",
      "3                   0                   0                   0   \n",
      "4                   3                   3                   1   \n",
      "\n",
      "   HOBBIES_1_007_TX_1  HOBBIES_1_008_TX_1  HOBBIES_1_009_TX_1  ...  \\\n",
      "0                   0                   2                   0  ...   \n",
      "1                   0                   0                   0  ...   \n",
      "2                   0                   8                   0  ...   \n",
      "3                   0                  10                   0  ...   \n",
      "4                   1                   0                   1  ...   \n",
      "\n",
      "   FOODS_3_823_TX_3_1d_lag_7d_MA  FOODS_3_823_TX_3_1d_lag_14d_MA  \\\n",
      "0                            NaN                             NaN   \n",
      "1                            NaN                             NaN   \n",
      "2                            NaN                             NaN   \n",
      "3                            NaN                             NaN   \n",
      "4                            NaN                             NaN   \n",
      "\n",
      "   FOODS_3_824_TX_3_1d_lag_7d_MA  FOODS_3_824_TX_3_1d_lag_14d_MA  \\\n",
      "0                            NaN                             NaN   \n",
      "1                            NaN                             NaN   \n",
      "2                            NaN                             NaN   \n",
      "3                            NaN                             NaN   \n",
      "4                            NaN                             NaN   \n",
      "\n",
      "   FOODS_3_825_TX_3_1d_lag_7d_MA  FOODS_3_825_TX_3_1d_lag_14d_MA  \\\n",
      "0                            NaN                             NaN   \n",
      "1                            NaN                             NaN   \n",
      "2                            NaN                             NaN   \n",
      "3                            NaN                             NaN   \n",
      "4                            NaN                             NaN   \n",
      "\n",
      "   FOODS_3_826_TX_3_1d_lag_7d_MA  FOODS_3_826_TX_3_1d_lag_14d_MA  \\\n",
      "0                            NaN                             NaN   \n",
      "1                            NaN                             NaN   \n",
      "2                            NaN                             NaN   \n",
      "3                            NaN                             NaN   \n",
      "4                            NaN                             NaN   \n",
      "\n",
      "   FOODS_3_827_TX_3_1d_lag_7d_MA  FOODS_3_827_TX_3_1d_lag_14d_MA  \n",
      "0                            NaN                             NaN  \n",
      "1                            NaN                             NaN  \n",
      "2                            NaN                             NaN  \n",
      "3                            NaN                             NaN  \n",
      "4                            NaN                             NaN  \n",
      "\n",
      "[5 rows x 27442 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to add moving averages and lagged sales for a given column\n",
    "'''\n",
    "def add_features(df, column_name):\n",
    "    # Apply a 1-day lag\n",
    "    lagged_series = df[column_name].shift(1)\n",
    "    \n",
    "    # Directly calculate the 7-day and 14-day rolling averages from the 1-day lagged series\n",
    "    # without storing the lagged series as a separate column in the dataframe\n",
    "    df[column_name + '_1d_lag_7d_MA'] = lagged_series.rolling(window=7).mean()\n",
    "    df[column_name + '_1d_lag_14d_MA'] = lagged_series.rolling(window=14).mean()\n",
    "\n",
    "# Iterate over each product column to add the modified features\n",
    "for column in sales_train_df_transposed.columns:\n",
    "    if column != 'd':  # Assuming 'd' is the column you want to exclude\n",
    "        add_features(sales_train_df_transposed, column)\n",
    "        \n",
    "'''\n",
    "    \n",
    "\n",
    "def add_features_efficiently(df, column_names):\n",
    "    # Create a temporary DataFrame to store all new features\n",
    "    new_features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        if column_name != 'd':  # Assuming 'd' is the column you want to exclude\n",
    "            # Apply a 1-day lag\n",
    "            lagged_series = df[column_name].shift(1)\n",
    "\n",
    "            \n",
    "            # Calculate the 7-day and 14-day rolling averages from the 1-day lagged series\n",
    "            new_features[column_name + '_1d_lag_7d_MA'] = lagged_series.rolling(window=7).mean()\n",
    "            new_features[column_name + '_1d_lag_14d_MA'] = lagged_series.rolling(window=14).mean()\n",
    "    \n",
    "    # Concatenate the new features with the original DataFrame\n",
    "    return pd.concat([df, new_features], axis=1)\n",
    "\n",
    "\n",
    "column_names = sales_train_df_transposed.columns\n",
    "sales_train_df_transposed = add_features_efficiently(sales_train_df_transposed, column_names)\n",
    "\n",
    "\n",
    "# Verify the results\n",
    "print(sales_train_df_transposed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2f28c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        d  HOBBIES_1_001_TX_1  HOBBIES_1_002_TX_1  HOBBIES_1_003_TX_1  \\\n",
      "0  d_1549                   1                   0                   0   \n",
      "1  d_1550                   0                   1                   0   \n",
      "2  d_1551                   0                   0                   0   \n",
      "3  d_1552                   3                   0                   0   \n",
      "4  d_1553                   0                   0                   0   \n",
      "\n",
      "   HOBBIES_1_004_TX_1  HOBBIES_1_005_TX_1  HOBBIES_1_006_TX_1  \\\n",
      "0                   4                   2                   3   \n",
      "1                   0                   0                   0   \n",
      "2                   4                   0                   2   \n",
      "3                   4                   0                   0   \n",
      "4                   0                   2                   0   \n",
      "\n",
      "   HOBBIES_1_007_TX_1  HOBBIES_1_008_TX_1  HOBBIES_1_009_TX_1  ...  \\\n",
      "0                   1                   6                   0  ...   \n",
      "1                   0                   0                   1  ...   \n",
      "2                   0                   7                   3  ...   \n",
      "3                   0                   1                   0  ...   \n",
      "4                   0                   0                   1  ...   \n",
      "\n",
      "   FOODS_3_823_TX_3_1d_lag_7d_MA  FOODS_3_823_TX_3_1d_lag_14d_MA  \\\n",
      "0                       0.142857                        0.500000   \n",
      "1                       0.142857                        0.428571   \n",
      "2                       0.142857                        0.357143   \n",
      "3                       0.142857                        0.285714   \n",
      "4                       0.285714                        0.357143   \n",
      "\n",
      "   FOODS_3_824_TX_3_1d_lag_7d_MA  FOODS_3_824_TX_3_1d_lag_14d_MA  \\\n",
      "0                            0.0                             0.0   \n",
      "1                            0.0                             0.0   \n",
      "2                            0.0                             0.0   \n",
      "3                            0.0                             0.0   \n",
      "4                            0.0                             0.0   \n",
      "\n",
      "   FOODS_3_825_TX_3_1d_lag_7d_MA  FOODS_3_825_TX_3_1d_lag_14d_MA  \\\n",
      "0                       0.857143                        0.928571   \n",
      "1                       0.857143                        1.000000   \n",
      "2                       0.857143                        0.857143   \n",
      "3                       1.000000                        0.785714   \n",
      "4                       0.857143                        0.785714   \n",
      "\n",
      "   FOODS_3_826_TX_3_1d_lag_7d_MA  FOODS_3_826_TX_3_1d_lag_14d_MA  \\\n",
      "0                       2.142857                        1.571429   \n",
      "1                       2.000000                        1.571429   \n",
      "2                       2.000000                        1.857143   \n",
      "3                       1.571429                        1.857143   \n",
      "4                       1.428571                        1.857143   \n",
      "\n",
      "   FOODS_3_827_TX_3_1d_lag_7d_MA  FOODS_3_827_TX_3_1d_lag_14d_MA  \n",
      "0                       1.857143                        1.928571  \n",
      "1                       1.714286                        1.357143  \n",
      "2                       1.142857                        1.357143  \n",
      "3                       1.000000                        1.285714  \n",
      "4                       1.000000                        1.214286  \n",
      "\n",
      "[5 rows x 27442 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove first 35 rows to filter data for past 365 days\n",
    "\n",
    "sales_train_df_transposed = sales_train_df_transposed.iloc[35:]\n",
    "\n",
    "# Reset the index after dropping the rows, if you want a continuous index starting from 0\n",
    "sales_train_df_transposed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the first few rows of the modified DataFrame to verify\n",
    "print(sales_train_df_transposed.head())\n",
    "\n",
    "# sales_train_df_transposed.to_csv('sales_t.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7540174",
   "metadata": {},
   "source": [
    "# Calendar Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6a955a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/cent7/jupyterhub/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  wm_yr_wk    weekday  wday  month  year    d  event_name_1  \\\n",
      "0  2011-01-29     11101   Saturday     1      1  2011  d_1            30   \n",
      "1  2011-01-30     11101     Sunday     2      1  2011  d_2            30   \n",
      "2  2011-01-31     11101     Monday     3      1  2011  d_3            30   \n",
      "3  2011-02-01     11101    Tuesday     4      2  2011  d_4            30   \n",
      "4  2011-02-02     11101  Wednesday     5      2  2011  d_5            30   \n",
      "\n",
      "   event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  snap_WI  \n",
      "0             4             4             2        0        0        0  \n",
      "1             4             4             2        0        0        0  \n",
      "2             4             4             2        0        0        0  \n",
      "3             4             4             2        1        1        0  \n",
      "4             4             4             2        1        0        1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the transform function\n",
    "def transform(calendar):\n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        calendar[feature].fillna('unknown', inplace=True)\n",
    "        \n",
    "    cat = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI']\n",
    "    for feature in cat:\n",
    "        encoder = LabelEncoder()\n",
    "        calendar[feature] = encoder.fit_transform(calendar[feature])\n",
    "    \n",
    "    return calendar\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "\n",
    "# Apply the transform function to the dataset\n",
    "transformed_data = transform(calendar_df)\n",
    "\n",
    "# Display the first few rows of the reordered DataFrame \n",
    "print(calendar_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2421c",
   "metadata": {},
   "source": [
    "# Merging Sales and Calendar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c00f2e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        d  wm_yr_wk  event_name_1  event_type_1  event_name_2  event_type_2  \\\n",
      "0  d_1549     11513            30             4             4             2   \n",
      "1  d_1550     11513            30             4             4             2   \n",
      "2  d_1551     11513            30             4             4             2   \n",
      "3  d_1552     11513            30             4             4             2   \n",
      "4  d_1553     11513            30             4             4             2   \n",
      "\n",
      "   snap_TX  HOBBIES_1_001_TX_1  HOBBIES_1_002_TX_1  HOBBIES_1_003_TX_1  ...  \\\n",
      "0        0                   1                   0                   0  ...   \n",
      "1        0                   0                   1                   0  ...   \n",
      "2        0                   0                   0                   0  ...   \n",
      "3        0                   3                   0                   0  ...   \n",
      "4        0                   0                   0                   0  ...   \n",
      "\n",
      "   FOODS_3_823_TX_3_1d_lag_7d_MA  FOODS_3_823_TX_3_1d_lag_14d_MA  \\\n",
      "0                       0.142857                        0.500000   \n",
      "1                       0.142857                        0.428571   \n",
      "2                       0.142857                        0.357143   \n",
      "3                       0.142857                        0.285714   \n",
      "4                       0.285714                        0.357143   \n",
      "\n",
      "   FOODS_3_824_TX_3_1d_lag_7d_MA  FOODS_3_824_TX_3_1d_lag_14d_MA  \\\n",
      "0                            0.0                             0.0   \n",
      "1                            0.0                             0.0   \n",
      "2                            0.0                             0.0   \n",
      "3                            0.0                             0.0   \n",
      "4                            0.0                             0.0   \n",
      "\n",
      "   FOODS_3_825_TX_3_1d_lag_7d_MA  FOODS_3_825_TX_3_1d_lag_14d_MA  \\\n",
      "0                       0.857143                        0.928571   \n",
      "1                       0.857143                        1.000000   \n",
      "2                       0.857143                        0.857143   \n",
      "3                       1.000000                        0.785714   \n",
      "4                       0.857143                        0.785714   \n",
      "\n",
      "   FOODS_3_826_TX_3_1d_lag_7d_MA  FOODS_3_826_TX_3_1d_lag_14d_MA  \\\n",
      "0                       2.142857                        1.571429   \n",
      "1                       2.000000                        1.571429   \n",
      "2                       2.000000                        1.857143   \n",
      "3                       1.571429                        1.857143   \n",
      "4                       1.428571                        1.857143   \n",
      "\n",
      "   FOODS_3_827_TX_3_1d_lag_7d_MA  FOODS_3_827_TX_3_1d_lag_14d_MA  \n",
      "0                       1.857143                        1.928571  \n",
      "1                       1.714286                        1.357143  \n",
      "2                       1.142857                        1.357143  \n",
      "3                       1.000000                        1.285714  \n",
      "4                       1.000000                        1.214286  \n",
      "\n",
      "[5 rows x 27448 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge the transposed sales data with the calendar data\n",
    "merged_df = pd.merge(sales_train_df_transposed, calendar_df, on='d', how='left')\n",
    "\n",
    "# Step 1: Identify the calendar columns (assuming these are all columns from 'date' onwards in the merged_df)\n",
    "calendar_columns = ['date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI']\n",
    "\n",
    "# Step 2: Construct the new column order\n",
    "new_column_order = ['d'] + calendar_columns + [col for col in merged_df.columns if col not in calendar_columns and col != 'd']\n",
    "\n",
    "# Step 3: Reorder the columns of merged_df\n",
    "merged_df = merged_df[new_column_order]\n",
    "\n",
    "# Drop non essential columns \n",
    "columns_to_remove = ['snap_CA', 'snap_WI', 'weekday', 'wday', 'month', 'year','date']\n",
    "\n",
    "# Remove the specified columns\n",
    "merged_df = merged_df.drop(columns=columns_to_remove)\n",
    "\n",
    "# Display the first few rows of the modified DataFrame to verify\n",
    "print(merged_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbcac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9147, 406)\n"
     ]
    }
   ],
   "source": [
    "dimensions = sales_train_df.shape\n",
    "print(dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d3e46",
   "metadata": {},
   "source": [
    "# Data Preprocessing of Sell Prices Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f43a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 6841121\n",
      "Number of Columns: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = 'sell_prices.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Get the dimensions of the DataFrame\n",
    "dimensions = df.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f'Number of Rows: {dimensions[0]}')\n",
    "print(f'Number of Columns: {dimensions[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a87fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        store_id        item_id  wm_yr_wk  sell_price\n",
      "2708822     TX_1  HOBBIES_1_001     11325        9.58\n",
      "2708823     TX_1  HOBBIES_1_001     11326        8.26\n",
      "2708824     TX_1  HOBBIES_1_001     11327        8.26\n",
      "2708825     TX_1  HOBBIES_1_001     11328        8.26\n",
      "2708826     TX_1  HOBBIES_1_001     11329        8.26\n",
      "                               id  wm_yr_wk  sell_price\n",
      "2708822  price_HOBBIES_1_001_TX_1     11325        9.58\n",
      "2708823  price_HOBBIES_1_001_TX_1     11326        8.26\n",
      "2708824  price_HOBBIES_1_001_TX_1     11327        8.26\n",
      "2708825  price_HOBBIES_1_001_TX_1     11328        8.26\n",
      "2708826  price_HOBBIES_1_001_TX_1     11329        8.26\n",
      "Number of Rows: 2092122\n",
      "Number of Columns: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "sell_prices_path = 'sell_prices.csv'\n",
    "calendar_data_path = 'calendar.csv'\n",
    "\n",
    "sell_prices_data = pd.read_csv(sell_prices_path)\n",
    "calendar_data = pd.read_csv(calendar_data_path)\n",
    "\n",
    "# Define the list of Texas store IDs\n",
    "texas_stores = ['TX_1', 'TX_2', 'TX_3']\n",
    "\n",
    "# Filter the sell_prices_data for only the specified Texas stores\n",
    "sell_prices_data = sell_prices_data[sell_prices_data['store_id'].isin(texas_stores)]\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame to verify\n",
    "print(sell_prices_data.head())\n",
    "\n",
    "# Step 1: Create a new \"id\" column in sell_prices_data by concatenating \"item_id\" and \"store_id\"\n",
    "sell_prices_data['id'] = 'price_'+sell_prices_data['item_id'] + '_' + sell_prices_data['store_id']\n",
    "\n",
    "# Step 2: Remove 'store_id', 'item_id', and 'wm_yr_wk' columns\n",
    "sell_prices_data.drop(['store_id', 'item_id'], axis=1, inplace=True)\n",
    "\n",
    "# Step 3: Bring the 'id' column to the front\n",
    "# Reorder the columns so 'id' is first\n",
    "columns = ['id'] + [col for col in sell_prices_data.columns if col != 'id']\n",
    "sell_prices_data = sell_prices_data[columns]\n",
    "\n",
    "print(sell_prices_data.head())\n",
    "\n",
    "# Get the dimensions of the DataFrame\n",
    "dimensions = sell_prices_data.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f'Number of Rows: {dimensions[0]}')\n",
    "print(f'Number of Columns: {dimensions[1]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a15fdcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wm_yr_wk                11101  11102  11103  11104  11105  11106  11107  \\\n",
      "id                                                                        \n",
      "price_FOODS_1_001_TX_1   2.00   2.00   2.00   2.00   2.00   2.00   2.00   \n",
      "price_FOODS_1_001_TX_2   2.00   2.00   2.00   2.00   2.00   2.00   2.00   \n",
      "price_FOODS_1_001_TX_3    NaN   2.00   2.00   2.00   2.00   2.00   2.00   \n",
      "price_FOODS_1_002_TX_1    NaN   7.88   7.88   7.88   7.88   7.88   7.88   \n",
      "price_FOODS_1_002_TX_2   7.88   7.88   7.88   7.88   7.88   7.88   7.88   \n",
      "\n",
      "wm_yr_wk                11108  11109  11110  ...  11612  11613  11614  11615  \\\n",
      "id                                           ...                               \n",
      "price_FOODS_1_001_TX_1   2.00   2.00   2.00  ...   2.24   2.24   2.24   2.24   \n",
      "price_FOODS_1_001_TX_2   2.00   2.00   2.00  ...   2.24   2.24   2.24   2.24   \n",
      "price_FOODS_1_001_TX_3   2.00   2.00   2.00  ...   2.24   2.24   2.24   2.24   \n",
      "price_FOODS_1_002_TX_1   7.88   7.88   7.88  ...   9.48   9.48   9.48   9.48   \n",
      "price_FOODS_1_002_TX_2   7.88   7.88   7.88  ...   9.48   9.48   9.48   9.48   \n",
      "\n",
      "wm_yr_wk                11616  11617  11618  11619  11620  11621  \n",
      "id                                                                \n",
      "price_FOODS_1_001_TX_1   2.24   2.24   2.24   2.24   2.24   2.24  \n",
      "price_FOODS_1_001_TX_2   2.24   2.24   2.24   2.24   2.24   2.24  \n",
      "price_FOODS_1_001_TX_3   2.24   2.24   2.24   2.24   2.24   2.24  \n",
      "price_FOODS_1_002_TX_1   9.48   9.48   9.48   9.48   9.48   9.48  \n",
      "price_FOODS_1_002_TX_2   9.48   9.48   9.48   9.48   9.48   9.48  \n",
      "\n",
      "[5 rows x 282 columns]\n",
      "Number of Rows: 9147\n",
      "Number of Columns: 282\n"
     ]
    }
   ],
   "source": [
    "# Pivot the DataFrame to make 'd' values columns, 'id' as the index, and 'sell_price' as the values\n",
    "pivot_df = sell_prices_data.pivot(index='id', columns='wm_yr_wk', values='sell_price')\n",
    "\n",
    "# Display the first few rows of the pivoted DataFrame\n",
    "print(pivot_df.head())\n",
    "\n",
    "# Get the dimensions of the DataFrame\n",
    "dimensions = pivot_df.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f'Number of Rows: {dimensions[0]}')\n",
    "print(f'Number of Columns: {dimensions[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "678f4c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id  wm_yr_wk  price_FOODS_1_001_TX_1  price_FOODS_1_001_TX_2  \\\n",
      "0      11101                     2.0                     2.0   \n",
      "1      11102                     2.0                     2.0   \n",
      "2      11103                     2.0                     2.0   \n",
      "3      11104                     2.0                     2.0   \n",
      "4      11105                     2.0                     2.0   \n",
      "\n",
      "id  price_FOODS_1_001_TX_3  price_FOODS_1_002_TX_1  price_FOODS_1_002_TX_2  \\\n",
      "0                      NaN                     NaN                    7.88   \n",
      "1                      2.0                    7.88                    7.88   \n",
      "2                      2.0                    7.88                    7.88   \n",
      "3                      2.0                    7.88                    7.88   \n",
      "4                      2.0                    7.88                    7.88   \n",
      "\n",
      "id  price_FOODS_1_002_TX_3  price_FOODS_1_003_TX_1  price_FOODS_1_003_TX_2  \\\n",
      "0                     7.88                    2.88                    2.88   \n",
      "1                     7.88                    2.88                    2.88   \n",
      "2                     7.88                    2.88                    2.88   \n",
      "3                     7.88                    2.88                    2.88   \n",
      "4                     7.88                    2.88                    2.88   \n",
      "\n",
      "id  price_FOODS_1_003_TX_3  ...  price_HOUSEHOLD_2_513_TX_3  \\\n",
      "0                     2.88  ...                         NaN   \n",
      "1                     2.88  ...                         NaN   \n",
      "2                     2.88  ...                         NaN   \n",
      "3                     2.88  ...                         NaN   \n",
      "4                     2.88  ...                         NaN   \n",
      "\n",
      "id  price_HOUSEHOLD_2_514_TX_1  price_HOUSEHOLD_2_514_TX_2  \\\n",
      "0                          NaN                       18.97   \n",
      "1                        18.97                       18.97   \n",
      "2                        18.97                       18.97   \n",
      "3                        18.97                       18.97   \n",
      "4                        18.97                       18.97   \n",
      "\n",
      "id  price_HOUSEHOLD_2_514_TX_3  price_HOUSEHOLD_2_515_TX_1  \\\n",
      "0                        18.97                         NaN   \n",
      "1                        18.97                         NaN   \n",
      "2                        18.97                         NaN   \n",
      "3                        18.97                         NaN   \n",
      "4                        18.97                         NaN   \n",
      "\n",
      "id  price_HOUSEHOLD_2_515_TX_2  price_HOUSEHOLD_2_515_TX_3  \\\n",
      "0                          NaN                         NaN   \n",
      "1                          NaN                         NaN   \n",
      "2                          NaN                         NaN   \n",
      "3                          NaN                         NaN   \n",
      "4                          NaN                         NaN   \n",
      "\n",
      "id  price_HOUSEHOLD_2_516_TX_1  price_HOUSEHOLD_2_516_TX_2  \\\n",
      "0                          NaN                        5.94   \n",
      "1                          NaN                        5.94   \n",
      "2                         5.94                        5.94   \n",
      "3                         5.94                        5.94   \n",
      "4                         5.94                        5.94   \n",
      "\n",
      "id  price_HOUSEHOLD_2_516_TX_3  \n",
      "0                         5.94  \n",
      "1                         5.94  \n",
      "2                         5.94  \n",
      "3                         5.94  \n",
      "4                         5.94  \n",
      "\n",
      "[5 rows x 9148 columns]\n",
      "Number of Rows: 282\n",
      "Number of Columns: 9148\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transpose the pivoted DataFrame\n",
    "transposed_df = pivot_df.T\n",
    "\n",
    "# Clear the name of the index to remove 'id' label from the top left corner\n",
    "transposed_df.index.name = None\n",
    "\n",
    "# Reset the index to turn the index into a regular column, then rename\n",
    "transposed_df_reset = transposed_df.reset_index()\n",
    "transposed_df_reset.rename(columns={'index': 'wm_yr_wk'}, inplace=True)\n",
    "\n",
    "\n",
    "print(transposed_df_reset.head())\n",
    "\n",
    "\n",
    "# Get the dimensions of the DataFrame\n",
    "dimensions = transposed_df_reset.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f'Number of Rows: {dimensions[0]}')\n",
    "print(f'Number of Columns: {dimensions[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd00ee",
   "metadata": {},
   "source": [
    "# Merging Sell price data and Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd16946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        d  wm_yr_wk  event_name_1  event_type_1  event_name_2  event_type_2  \\\n",
      "0  d_1549     11513            30             4             4             2   \n",
      "1  d_1550     11513            30             4             4             2   \n",
      "2  d_1551     11513            30             4             4             2   \n",
      "3  d_1552     11513            30             4             4             2   \n",
      "4  d_1553     11513            30             4             4             2   \n",
      "\n",
      "   snap_TX  HOBBIES_1_001_TX_1  HOBBIES_1_002_TX_1  HOBBIES_1_003_TX_1  ...  \\\n",
      "0        0                   1                   0                   0  ...   \n",
      "1        0                   0                   1                   0  ...   \n",
      "2        0                   0                   0                   0  ...   \n",
      "3        0                   3                   0                   0  ...   \n",
      "4        0                   0                   0                   0  ...   \n",
      "\n",
      "   price_HOUSEHOLD_2_513_TX_3  price_HOUSEHOLD_2_514_TX_1  \\\n",
      "0                        2.78                       19.54   \n",
      "1                        2.78                       19.54   \n",
      "2                        2.78                       19.54   \n",
      "3                        2.78                       19.54   \n",
      "4                        2.78                       19.54   \n",
      "\n",
      "   price_HOUSEHOLD_2_514_TX_2  price_HOUSEHOLD_2_514_TX_3  \\\n",
      "0                       19.54                       19.54   \n",
      "1                       19.54                       19.54   \n",
      "2                       19.54                       19.54   \n",
      "3                       19.54                       19.54   \n",
      "4                       19.54                       19.54   \n",
      "\n",
      "   price_HOUSEHOLD_2_515_TX_1  price_HOUSEHOLD_2_515_TX_2  \\\n",
      "0                        1.97                        1.97   \n",
      "1                        1.97                        1.97   \n",
      "2                        1.97                        1.97   \n",
      "3                        1.97                        1.97   \n",
      "4                        1.97                        1.97   \n",
      "\n",
      "   price_HOUSEHOLD_2_515_TX_3  price_HOUSEHOLD_2_516_TX_1  \\\n",
      "0                        1.97                        5.94   \n",
      "1                        1.97                        5.94   \n",
      "2                        1.97                        5.94   \n",
      "3                        1.97                        5.94   \n",
      "4                        1.97                        5.94   \n",
      "\n",
      "   price_HOUSEHOLD_2_516_TX_2  price_HOUSEHOLD_2_516_TX_3  \n",
      "0                        5.94                        5.94  \n",
      "1                        5.94                        5.94  \n",
      "2                        5.94                        5.94  \n",
      "3                        5.94                        5.94  \n",
      "4                        5.94                        5.94  \n",
      "\n",
      "[5 rows x 36595 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming merged_df and transposed_df_reset are already defined and ready\n",
    "\n",
    "# Step 1: Identify the position of 'snap_TX' in merged_data to know where to insert new columns\n",
    "snap_WI_position = merged_df.columns.get_loc(\"snap_TX\") + 1\n",
    "\n",
    "# Step 2: Merge the DataFrames on 'wm_yr_wk'\n",
    "# Note: This is a simplification. You might need a left, right, or outer join depending on your data context.\n",
    "merged_final = pd.merge(merged_df, transposed_df_reset, on=\"wm_yr_wk\", how=\"left\")\n",
    "\n",
    "'''\n",
    "# Step 3: Reorder columns to ensure new columns are after 'snap_WI', if necessary\n",
    "# This step is somewhat complex because it requires dynamically adjusting the column order based on merge results\n",
    "\n",
    "# Get a list of all column names\n",
    "columns = list(merged_final.columns)\n",
    "\n",
    "# Identify the columns that came from transposed_df_reset (assuming they're not in merged_data originally)\n",
    "new_columns = [col for col in transposed_df_reset.columns if col not in merged_df.columns and col != 'wm_yr_wk']\n",
    "\n",
    "# Reorganize columns: keep everything up to 'snap_WI' in place, insert new_columns, then append the rest\n",
    "organized_columns = columns[:snap_WI_position] + new_columns + columns[snap_WI_position:len(columns)-len(new_columns)]\n",
    "\n",
    "# Reassign column order\n",
    "merged_final = merged_final[organized_columns]\n",
    "\n",
    "'''\n",
    "\n",
    "# Verify the merge and column order\n",
    "print(merged_final.head())\n",
    "\n",
    "merged_final.to_csv('merged_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca6d28",
   "metadata": {},
   "source": [
    "# Downcasting of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00b40b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02e78b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        d  wm_yr_wk  event_name_1  event_type_1  event_name_2  event_type_2  \\\n",
      "0  d_1549     11513            30             4             4             2   \n",
      "1  d_1550     11513            30             4             4             2   \n",
      "2  d_1551     11513            30             4             4             2   \n",
      "3  d_1552     11513            30             4             4             2   \n",
      "4  d_1553     11513            30             4             4             2   \n",
      "\n",
      "   snap_TX  HOBBIES_1_001_TX_1  HOBBIES_1_002_TX_1  HOBBIES_1_003_TX_1  ...  \\\n",
      "0        0                   1                   0                   0  ...   \n",
      "1        0                   0                   1                   0  ...   \n",
      "2        0                   0                   0                   0  ...   \n",
      "3        0                   3                   0                   0  ...   \n",
      "4        0                   0                   0                   0  ...   \n",
      "\n",
      "   price_HOUSEHOLD_2_513_TX_3  price_HOUSEHOLD_2_514_TX_1  \\\n",
      "0                        2.78                   19.540001   \n",
      "1                        2.78                   19.540001   \n",
      "2                        2.78                   19.540001   \n",
      "3                        2.78                   19.540001   \n",
      "4                        2.78                   19.540001   \n",
      "\n",
      "   price_HOUSEHOLD_2_514_TX_2  price_HOUSEHOLD_2_514_TX_3  \\\n",
      "0                   19.540001                   19.540001   \n",
      "1                   19.540001                   19.540001   \n",
      "2                   19.540001                   19.540001   \n",
      "3                   19.540001                   19.540001   \n",
      "4                   19.540001                   19.540001   \n",
      "\n",
      "   price_HOUSEHOLD_2_515_TX_1  price_HOUSEHOLD_2_515_TX_2  \\\n",
      "0                        1.97                        1.97   \n",
      "1                        1.97                        1.97   \n",
      "2                        1.97                        1.97   \n",
      "3                        1.97                        1.97   \n",
      "4                        1.97                        1.97   \n",
      "\n",
      "   price_HOUSEHOLD_2_515_TX_3  price_HOUSEHOLD_2_516_TX_1  \\\n",
      "0                        1.97                        5.94   \n",
      "1                        1.97                        5.94   \n",
      "2                        1.97                        5.94   \n",
      "3                        1.97                        5.94   \n",
      "4                        1.97                        5.94   \n",
      "\n",
      "   price_HOUSEHOLD_2_516_TX_2  price_HOUSEHOLD_2_516_TX_3  \n",
      "0                        5.94                        5.94  \n",
      "1                        5.94                        5.94  \n",
      "2                        5.94                        5.94  \n",
      "3                        5.94                        5.94  \n",
      "4                        5.94                        5.94  \n",
      "\n",
      "[5 rows x 36595 columns]\n",
      "Number of Rows: 365\n",
      "Number of Columns: 36595\n"
     ]
    }
   ],
   "source": [
    "def downcasting(df):\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int16)\n",
    "    return df\n",
    "\n",
    "merged_final=downcasting(merged_final)\n",
    "\n",
    "\n",
    "# Verify the merge and column order\n",
    "print(merged_final.head())\n",
    "\n",
    "# Get the dimensions of the DataFrame\n",
    "dimensions = merged_final.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f'Number of Rows: {dimensions[0]}')\n",
    "print(f'Number of Columns: {dimensions[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806eecd1",
   "metadata": {},
   "source": [
    "# Saving data as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22ec0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final = merged_final.drop(columns=['wm_yr_wk'])\n",
    "merged_final.to_csv('merged_data_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7008b",
   "metadata": {},
   "source": [
    "# Part 2 Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy pandas scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9dc018",
   "metadata": {},
   "source": [
    "# Tensorflow memory growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7572872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable TensorFlow's memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set at program startup\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdd825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddee18d",
   "metadata": {},
   "source": [
    "# Data preprocessing and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90472ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally enable mixed precision\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# The rest of your code for loading data, preprocessing, model definition, and training goes here\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('merged_data_final.csv')\n",
    "\n",
    "# Assuming the first column is the day and the rest are features\n",
    "features = df.iloc[:, 1:].values\n",
    "\n",
    "# Assuming 'features' is your DataFrame\n",
    "features_df = pd.DataFrame(features)\n",
    "\n",
    "# Identify and drop constant columns\n",
    "constant_columns = features_df.columns[features_df.nunique() <= 1]\n",
    "features_df.drop(constant_columns, axis=1, inplace=True)\n",
    "\n",
    "features_df_new = features_df.dropna(axis=1)\n",
    "\n",
    "# If you want to drop columns that are entirely NaN, you can use the 'how' parameter\n",
    "# features_df_cleaned = features_df.dropna(axis=1, how='all')\n",
    "\n",
    "# Show the shape of the original and cleaned DataFrames as a quick check\n",
    "print(\"Original shape:\", features_df.shape)\n",
    "print(\"Cleaned shape:\", features_df_new.shape)\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler.fit_transform(features_df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b264d1ae",
   "metadata": {},
   "source": [
    "# LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Reshape, TimeDistributed\n",
    "\n",
    "\n",
    "def create_dataset(data, look_back=28, forecast_horizon=28):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - look_back - forecast_horizon + 1):\n",
    "        a = data[i:(i + look_back)]\n",
    "        X.append(a)\n",
    "        Y.append(data[(i + look_back):(i + look_back + forecast_horizon)])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "look_back = 45\n",
    "forecast_horizon = 28\n",
    "X, Y = create_dataset(features_scaled, look_back, forecast_horizon)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "Y_train, Y_test = Y[:split], Y[split:]\n",
    "\n",
    "# No need to reshape X_train and X_test as they are already in the correct shape\n",
    "# However, you might want to ensure Y_train and Y_test are correctly shaped for your model, especially if using LSTM layers for output\n",
    "\n",
    "\n",
    "# Adjusting the model architecture\n",
    "number_of_features = 29937\n",
    "forecast_horizon = 28\n",
    "look_back = 45\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(look_back, X_train.shape[2]), return_sequences=True),\n",
    "    LSTM(50),\n",
    "    # Use Reshape or TimeDistributed layer to adjust for the output shape (28 days, 29937 features each day)\n",
    "    # Adding a Dense layer with the number of outputs you need for each time step, wrapped in a TimeDistributed layer\n",
    "    # to apply it across each of the 28 time steps\n",
    "    Dense(forecast_horizon * number_of_features, activation='linear'),\n",
    "    # Reshape the output to the desired format: [samples, time steps, features]\n",
    "    Reshape((forecast_horizon, number_of_features))\n",
    "])\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Note: With such a large output dimension, training this model might require significant computational resources.\n",
    "# Ensure that your hardware is capable of handling this complexity.\n",
    "\n",
    "# Update the model's weights to float16 for mixed precision training\n",
    "model.summary()  # Check if the model uses mixed precision\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "\n",
    "# Reduce batch size to decrease memory consumption\n",
    "batch_size = 32  # Reduced batch size\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=batch_size, validation_split=0.1, callbacks=[early_stopping], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_X_test = X_test.shape\n",
    "print(f\"Shape of X_test: {shape_X_test}\")\n",
    "\n",
    "# Find the shape of Y_test\n",
    "shape_Y_test = Y_test.shape\n",
    "print(f\"Shape of Y_test: {shape_Y_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2639b9",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2866d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Predicting the features for the test set\n",
    "predicted_features = model.predict(X_test)\n",
    "\n",
    "# Assuming `predicted_features` and `Y_test` are now correctly inverse transformed and have shapes [samples, 28, 29937]\n",
    "\n",
    "# Flatten the 3D arrays to 2D arrays\n",
    "predicted_features_flat = predicted_features.reshape(-1, predicted_features.shape[1]*predicted_features.shape[2])\n",
    "Y_test_flat = Y_test.reshape(-1, Y_test.shape[1]*Y_test.shape[2])\n",
    "\n",
    "# Now both arrays are 2D: [samples, 28*29937]\n",
    "\n",
    "# Calculating RMSE\n",
    "rmse = sqrt(mean_squared_error(Y_test_flat, predicted_features_flat))\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "\n",
    "# Optionally, calculate additional metrics like MAE in a similar flattened manner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Function to inverse transform 3D data\n",
    "def inverse_transform_3d(scaler, data):\n",
    "    # Reshape data from 3D to 2D to apply inverse transformation\n",
    "    data_reshaped = data.reshape(-1, data.shape[2])\n",
    "    data_inverse = scaler.inverse_transform(data_reshaped)\n",
    "    # Reshape back to 3D\n",
    "    data_inverse_3d = data_inverse.reshape(data.shape)\n",
    "    return data_inverse_3d\n",
    "\n",
    "# Apply inverse transformation to predictions and actual values\n",
    "predicted_features_inv = inverse_transform_3d(scaler, predicted_features)\n",
    "Y_test_inv = inverse_transform_3d(scaler, Y_test)\n",
    "\n",
    "# Flatten the 3D arrays to 2D for overall RMSE calculation\n",
    "predicted_features_flat = predicted_features_inv.reshape(-1, predicted_features_inv.shape[1]*predicted_features_inv.shape[2])\n",
    "Y_test_flat = Y_test_inv.reshape(-1, Y_test_inv.shape[1]*Y_test_inv.shape[2])\n",
    "\n",
    "# Calculating RMSE\n",
    "rmse = sqrt(mean_squared_error(Y_test_flat, predicted_features_flat))\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed4919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
